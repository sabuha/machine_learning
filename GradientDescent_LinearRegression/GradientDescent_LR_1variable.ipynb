{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook shows a simplified application of the gradient descent algorithm, to fit a regression model with one parameter (i.e. finding the slope of the regression line to get the best estimation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists in a sample of 20 observations. For each individual, we have the age, the weight and the systolic pressure. Column \"age\" is removed from dataframe for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"systolic_blood_press.csv\")\n",
    "del df['age']\n",
    "display(df.head())\n",
    "display(df.describe())\n",
    "m = len(df) # number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot between weight and systolic pressure shows a strong correlation. This can be modelized with a regression line. In this example, the intercept is deliberately null to keep the example simple. The instruction <i>sm.OLS</i> finds the best regression line slope so that the SSR (sum of squared residuals) is the lowest possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit regression model\n",
    "plt.scatter(x=df.weight, y=df.systolic_press)\n",
    "model=sm.OLS(df.systolic_press, df.weight) #no intercept\n",
    "res=model.fit()\n",
    "slope=res.params['weight']\n",
    "\n",
    "# Plot the regression line\n",
    "abline_values = [slope*i for i in df.weight]\n",
    "plt.plot(df.weight, abline_values, color=\"red\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Systolic Pressure\")\n",
    "\n",
    "# Values found by stats_model\n",
    "print(\"Slope from statsmodels OLS:\", slope)\n",
    "print(\"SSR from statsmodels\", res.ssr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more the line fits, the less SSR is, and vice versa. This can be expressed as a cost function. This last takes domain of parameter to find in input (here the slope of the regression line), and outputs the corresponding SSR. This function has a minimum where x is the best parameter, and y the minimum corresponding SSR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost_function(coefficient):\n",
    "    error_squared=0\n",
    "    # iterate through the sample and sum the squares of the distance between each point to the regression line\n",
    "    for row in df.itertuples():\n",
    "        index, systolic_press, weight = row\n",
    "        estimated_y=coefficient*weight\n",
    "        error_squared += np.square(systolic_press-estimated_y)\n",
    "    return error_squared/len(df)\n",
    "\n",
    "# Visualize the cost function\n",
    "cost_x = np.arange(slope-0.5, slope+0.55, 0.05)\n",
    "cost_y = [cost_function(i) for i in cost_x]\n",
    "plt.plot(cost_x, cost_y)\n",
    "plt.xlabel(\"variable to find\")\n",
    "plt.ylabel(\"SSR\")\n",
    "\n",
    "print(\"SSR returned by the cost function:\", cost_function(slope)*m)\n",
    "print(\"SSR from statsmodels:\", res.ssr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the point of gradient descent is to find this minimum. Because the cost function is convex, it has a unique minimum which is local and global. Thus, one could use its derivative to find its minimum. Gradient descent starts with an initial guess and improves it at each iteration, so that it tends to the value minimizing the cost function. While approaching the minimum, the slope tends to null, and gradients are smaller and smaller (convergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_iter(min_x):\n",
    "    # if alpha is too big, the algorithm will not converge and \"jump\" above the minimum\n",
    "    alpha = 0.0001\n",
    "    epsilon = 0.00001\n",
    "    max_iteration = 100 #in case of no convergence (alpha too big)\n",
    "    iter = 0\n",
    "    while True:\n",
    "        iter += 1\n",
    "        # at each gradient, it iterates through the sample (sum(..), not efficient on large samples)\n",
    "        derivative = sum([(min_x*df.weight[i] - df.systolic_press[i])*df.weight[i] for i in range(m)]) / m\n",
    "        min_x = min_x - (alpha*derivative)\n",
    "        if (abs(derivative) < epsilon) or (iter > max_iteration):\n",
    "            return min_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_x = gradient_descent_iter(0)\n",
    "print(\"Found by gradient descent:\", min_x)\n",
    "print(\"From Statsmodels:\", slope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
